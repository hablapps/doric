{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<<<<<< local\n",
    "<img src=\"images/dorict.png\" alt=\"drawing\" width=\"200\"/>\n",
    "=======\n",
    "<img src=\"dorict.png\" alt=\"drawing\" width=\"200\"/>\n",
    ">>>>>>> remote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<<<<<< LOCAL CELL DELETED >>>>>>>\n",
    "<img src=\"images/dorict.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.1.1`\n",
    "import $ivy.`org.typelevel::cats-core:2.3.0`\n",
    "import $ivy.`com.lihaoyi::sourcecode:0.2.6`\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.{functions => f}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is no problem in combining conventional Spark column expressions and doric columns. However, to avoid name clashes, we will use the prefix `f` for the former ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = org.apache.spark.sql.SparkSession.builder().appName(\"test\").master(\"local\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doric setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding sonatype snapshot repository - proper versions coming soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coursierapi.MavenRepository\n",
    "\n",
    "interp.repositories.update(\n",
    "  interp.repositories() ::: List(MavenRepository.of(\"https://oss.sonatype.org/content/repositories/snapshots\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.hablapps::doric:0.0.0+83-2a1f5312-SNAPSHOT`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific _doric_ imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import habla.doric._\n",
    "import habla.doric.functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some compelling reasons for using `doric`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it's a list of use cases for doric: \n",
    "* Case 1: Get rid of malformed column expressions at compile time \n",
    "* Case 2: Avoid implicit type castings\n",
    "* Case 3: Run DataFrames only when it is safe to do so\n",
    "* Case 4: Get all errors at once\n",
    "* Case 5: Modularize your business logic \n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<<<<<< REMOTE CELL DELETED >>>>>>>\n",
    "Here it's a list of use cases for doric: \n",
    "* Case 1: Get rid of malformed column expressions at compile time \n",
    "* Case 2: Avoid implicit type castings\n",
    "* Case 3: Run DataFrames only when it is safe to do so\n",
    "* Case 4: Get all errors at once\n",
    "* Case 5: Modularize your business logic \n",
    "\n",
    "As we will see shortly, you will achieve all these goodies: \n",
    "<<<<<<< local\n",
    "* Without resorting to Datasets and sacrifying performance, i.e. sticking to DataFrames\n",
    "* With minimal change in your code with respect conventional column expressions\n",
    "<<<<<<< local\n",
    "<<<<<<< local\n",
    "=======\n",
    "* Without resorting to Datasets and sacrificing performance, i.e. sticking to DataFrames\n",
    "* With minimal change in your code with respect to conventional column expressions\n",
    ">>>>>>> remote\n",
    "* Without fully committing to a strong static typing discipline throughout your code\n",
    "=======\n",
    "* Without fully committing to a strong static typing discipline in all your code\n",
    ">>>>>>> remote\n",
    "=======\n",
    "* Without fully committing to a strong static typing discipline throughout your code\n",
    ">>>>>>> remote\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Get rid of malformed column expressions at compile time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't mix apples and oranges, and Spark knows that. For instance, Spark complains if we try to add integers with booleans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val df = List(1,2,3).toDF.select($\"value\" * f.lit(true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it complains too late, with an exception raised at runtime. If we delay the creation of the DataFrame, the error dissapears ...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df = List(1,2,3).toDF.select($\"value\" * f.lit(true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... momentarily, until we eventually invoke that code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using doric, there is no need to wait so long: errors will be reported at compile-time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This doesn't compile\n",
    "def df = List(1,2,3).toDF.select(col[Int](\"value\") * lit(true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes in column expressions are minimal: just annotate column references with the intended type, i.e. `col[Int](\"value\")`, instead of a plain `col(\"value\")`. If you are not used to generic parameters, aliases `colInt`, `colString`, etc., are also available. We will use these aliases in the sequel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Naturally, this only works if you, the programmer, know the intended type of the column at compile-time. In a pure dynamic setting, doric is useless. Note, however, that you don't need to know in advance the whole row type as with `Dataset`s. In this way, doric sits between a whole-hearted static setting and a purely dynamic one. It offers type-safety at a minimum cost, without compromising performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use doric columns within the context of a `withColumn` expression, or, in general, wherever we use plain columns: `join`, `filter`, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(1,2,3).toDF.withColumn(\"other\", colInt(\"value\") * lit(1))\n",
    "List(1,2,3).toDF.filter(colInt(\"value\") > lit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join expressions are explained in a separate [notebook](joins.ipynb) in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Explicitly avoid implicit type castings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicit type conversions in Spark are pervasive. For instance, the following code won't cause Spark to complain at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df0 = spark.range(1,10).withColumn(\"x\", f.concat(f.col(\"id\"), f.lit(\"jander\"))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means that an implicit conversion from integer to string is in effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.select(f.col(\"x\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that you are certain that your column holds vales of type `bigint`, the same code in doric won't compile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.range(1,10).toDF.withColumn(\"x\", concat(colLong(\"id\"), \"jander\".lit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the Spark type `bigint` corresponds to the Scala type `Long`. The correspondences between Spark and Scala types in doric is the same as the one established in `Dataset`s by `Encoder` instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, doric will allow you to perform that operation provided that you explicitly enact the conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.range(1,10).toDF.withColumn(\"x\", concat(colLong(\"id\").cast[String], \"jander\".lit))\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfEq = List((1, \"1\"), (1, \" 1\"), (1, \" 1 \")).toDF(\"int\", \"str\")\n",
    "dfEq.withColumn(\"eq\", f.col(\"int\") === f.col(\"str\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would you expect to be the result? Well, it all depends on the implicit conversion that Spark chooses to apply, if at all: 1) it may return false for the new column, given that the types of both input columns differ, thus choosing to apply no conversion; 2) it may convert the integer column into a string column; 3) it may convert strings to integers. Let's see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEq.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 3 wins, but you can only learn this by trial and error. With doric, you can depart from all this magic and explicitly cast types, if you desired so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Option 1, no castings: compile error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEq.withColumn(\"eq\", colInt(\"int\") === colString(\"str\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Option 2, casting from int to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEq.withColumn(\"eq\", colInt(\"int\").cast[String] === colString(\"str\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Option 3, casting from string to int, not safe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEq.withColumn(\"eq\", colInt(\"int\") === colString(\"str\").unsafeCast[Int]).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can't simply `cast` an string to an integer, since this conversion is partial. If the programmer insists in doing this unsafe casting, doric will force her to explicitly acknowledge this fact using the conversion function `unsafeCast`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It's all about being explicit in order to enhance readability and avoid unexpected behaviours at runtime. Doric is a coding accelerator!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Don't let your DataFrame run if it shouldn't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose that your DataFrame contains a reference to a non-existing column. No problem, Spark will detect that and will complain with an exception at runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "List(1,2,3).toDF.select(f.col(\"id\")+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's assume that the column exists but its type is not what we expected. Spark won't be able to detect that, since type expectations are not encoded in plain columns. Thus, the following code will compile and execute without errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = List(\"1\",\"2\",\"three\").toDF.select(f.col(\"value\") + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we will be able to run the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obtaining `null` values and garbage results, in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using doric we can prevent the creation of the DataFrame, since column expressions are typed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = List(\"1\",\"2\",\"three\").toDF.select(colInt(\"value\") + 1.lit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the column doesn't exist, it will complain with a similar message to that given by Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val df = List(\"1\",\"2\",\"three\").toDF.select(colInt(\"id\") + 1.lit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But note that the location of the error is also included. This will prove immensely useful, as we will see later on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Get all errors at once!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following DataFrame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfadd = List((1,2),(3,4)).toDF(\"int1\", \"int2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try to add both columns as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfadd.withColumn(\"add\", f.col(\"Int_1\") + f.col(\"Int_2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rightly, Spark complains because column \"Int_1\" doesn't exist. Let's fix that problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfadd.withColumn(\"add\", f.col(\"int1\") + f.col(\"Int_2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooops, another error. Fortunately, this is the last one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfadd.withColumn(\"add\", f.col(\"int1\") + f.col(\"int2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, why didn't Spark give us all errors at once? Well, a plain fail-fast strategy for error reporting is simpler. Unlike Spark, doric won't stop at the first error, and will keep accumulating all errors until no further one is found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfadd.withColumn(\"add\", colInt(\"int_1\") + colInt(\"int_2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Modularity FTW!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend that our business logic is very complex, and modularised in different functions. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val col1: Column = f.col(\"int_1\")\n",
    "val col2: Column = f.col(\"int2\")\n",
    "val addColumns: Column = col1 + col2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an error when referring to the first column and Spark reports it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfadd.withColumn(\"add\", addColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, Spark does not give a clue about the exact source of the error. It marks the error in the `withColumn` method, but the actual problem is elsewhere, in expression `col1`. You have no choice but diving into the code and perform a brute exhaustive search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using doric, we can modularise our code without remorse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val col1: DoricColumn[Int] = colInt(\"int_1\")\n",
    "val col2: DoricColumn[Int]  = colString(\"int2\").unsafeCast[Int]\n",
    "val addColumns: DoricColumn[Int] = col1 + col2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we attempt to compose the DataFrame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfadd.withColumn(\"add\", addColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will get not only the errors, but the exact location of the culprit: \n",
    "\n",
    "```\n",
    "habla.doric.DoricMultiError: Found 2 errors in withColumn\n",
    "\tCannot resolve column name \"int_1\" among (int1, int2)\n",
    "\t\tlocated at . (cmd83.sc:1)\n",
    "\tThe column with name 'int2' is of type IntegerType and it was expected to be StringType\n",
    "\t\tlocated at . (cmd83.sc:2)\n",
    "```\n",
    "\n",
    "<<<<<<< local\n",
    "<<<<<<< local\n",
    "As you can see, errors are reported referring to the source files (`cmd83.sc`) and line numbers (`1` and `2`, respectively) where they are located. If you are using an IDE, you will additionally obtain an hyperlink to the error. Isn't that nice? :)\n",
    "=======\n",
    "As you can see, errors are reported referring to the source files (`cmd83.sc`) and line numbers (`1` and `2`, respectively) where they are located. If you are using an IDE, you will additionally obtain an hyperlink to the error.\n",
    ">>>>>>> remote\n",
    "=======\n",
    "As you can see, errors are reported referring to the source files (`cmd83.sc`) and line numbers (`1` and `2`, respectively) where they are located. If you are using an IDE, you will additionally obtain an hyperlink to the error. Isn't that nice? :)\n",
    ">>>>>>> remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
