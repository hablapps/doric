{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bbf9e26",
   "metadata": {},
   "source": [
    "## Spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b695479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import coursierapi.MavenRepository\n",
    "interp.repositories.update(\n",
    "  interp.repositories() ::: List(MavenRepository.of(\"https://oss.sonatype.org/content/repositories/snapshots\"))\n",
    ")\n",
    "\n",
    "import $ivy.`org.apache.spark::spark-sql:3.1.1`\n",
    "import $ivy.`org.typelevel::cats-core:2.3.0`\n",
    "import $ivy.`com.lihaoyi::sourcecode:0.2.6`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.hablapps::doric:0.0.0+84-91fcaa9a-SNAPSHOT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e27294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.{functions => f}\n",
    "import doric._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = org.apache.spark.sql.SparkSession.builder().appName(\"test\").master(\"local\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc490bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb461a5",
   "metadata": {},
   "source": [
    "## Typed join expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d4670",
   "metadata": {},
   "source": [
    "Spark column expressions are subject to the same problems that we explored [previously](\"README.ipynb): lack of static typing, unreported errors at DataFrame compile-time, unsolicited implicit castings, etc. For instance, given the following DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d9d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "val leftdf = List((1, 1, \"hi\"), (2, 2, \"bye\"), (3, 3, \"3\")).toDF(\"id-left\", \"id\", \"value-left\")\n",
    "val rightdf = List((1, 1, \"hi\"), (2, 2, \"bye\"), (3, 3, \"3\")).toDF(\"id-right\", \"id\", \"value-right\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b020da",
   "metadata": {},
   "source": [
    "the following equi-join expressions, where we use different mechanisms to refer to the corresponding columns, compile and run without problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f0fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "leftdf.join(rightdf, f.col(\"id-left\") === f.col(\"id-right\"))\n",
    "leftdf.join(rightdf, leftdf(\"id\") === rightdf(\"id\"))\n",
    "leftdf.alias(\"left\").join(rightdf.alias(\"right\"), f.col(\"left.id\") === f.col(\"right.id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce498a34",
   "metadata": {},
   "source": [
    "However, if some column name or type is wrong, then errors will be shown too late, or implicit type casts will be applied. For instance, the following DataFrame compiles in Spark and runs with garbage results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfjoin = leftdf.join(rightdf, leftdf(\"id\") === rightdf(\"value-right\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278cc33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfjoin.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b4ab4e",
   "metadata": {},
   "source": [
    "Using doric, errors can be detected at (Scala) compile-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f30583",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Scala will prevent this from compiling successfully\n",
    "def dfjoin = leftdf.join(rightdf, LeftDF.colInt(\"id\") === RightDF.colString(\"value-right\"), \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23383a1b",
   "metadata": {},
   "source": [
    "or at DataFrame-construction time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da4f22c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val dfjoin = leftdf.join(rightdf, LeftDF.colInt(\"id-left\") === RightDF.colInt(\"value-right\"), \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23db125c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val dfjoin = leftdf.join(rightdf, LeftDF.colInt(\"id-left1\") === RightDF.colInt(\"value-right\"), \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e977c",
   "metadata": {},
   "source": [
    "As you can see, join doric expressions also enjoy all the goodies concerning error location :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b9ce4",
   "metadata": {},
   "source": [
    "If everything is well-typed, then the DataFrame constructed will be exactly the same than the one obtained using conventional column expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27df74b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "leftdf.join(rightdf, LeftDF.colInt(\"id\") === RightDF.colInt(\"id\"), \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8523a5b7",
   "metadata": {},
   "source": [
    "As all these examples show, _join_ doric column expressions refer to the left and right DataFrames using the special objects `LeftDF` and `RightDF`, respectively. We can also refer to arbitrarily complex column expressions within the context of the left and right DataFrames, enclosing the expression between parentheses. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd617e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val leftCol = LeftDF.colString(\"id2\")\n",
    "val rightCol = RightDF(/* complex column expression here*/ colInt(\"id\").cast[String])\n",
    "leftdf.join(rightdf, leftCol === rightCol, \"inner\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad977f4",
   "metadata": {},
   "source": [
    "and, as this example also shows, we can also decompose doric join expressions in different functions so as to obtain more modular designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae160d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3cb66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4234b202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
